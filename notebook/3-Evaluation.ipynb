{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Develop\\\\Pipline\\\\Face_recognitions\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Develop\\\\Pipline\\\\Face_recognitions'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Evaluation_Config:\n",
    "    test_faces_dir: Path\n",
    "    represent_file:Path\n",
    "    test_dir:Path\n",
    "    model_name: str\n",
    "    face_detector_backend: str\n",
    "    target_size: tuple\n",
    "    align: bool\n",
    "    enforce_detection: bool\n",
    "    expand_percentage: int\n",
    "    normalization:str\n",
    "    silent: bool\n",
    "    gray_scale: bool\n",
    "    distance_matrix: str\n",
    "    threshold:int\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src configaration management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_recognitions.constants import *\n",
    "from face_recognitions.utils.common import read_yaml,create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_YAML_PATH,\n",
    "                 params_filepath=PARAMS_YAML_PATH):\n",
    "        self.config=read_yaml(config_filepath)\n",
    "        self.params=read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_eval_config(self)->Evaluation_Config:\n",
    "        config=self.config.evaluation\n",
    "        represent_file=self.config.train_model.represent_file\n",
    "        params=self.params\n",
    "        create_directories([config.test_dir])\n",
    "\n",
    "        eval_config=Evaluation_Config(\n",
    "            test_dir=config.test_dir,\n",
    "            represent_file=represent_file,\n",
    "            test_faces_dir=config.test_faces_dir,\n",
    "            model_name= params.MODEL_NAME,\n",
    "            face_detector_backend=params.FACE_DETECTOR_BACKEND,\n",
    "            target_size=params.TARGET_SIZE,\n",
    "            align=params.ALIGN,\n",
    "            enforce_detection=params.ENFORCE_DETECTION,\n",
    "            expand_percentage=params.EXPAND_PERCENTAGE,\n",
    "            normalization=params.NORMALIZATION,\n",
    "            silent=params.SILENT,\n",
    "            gray_scale=params.GRAY_SCALE,\n",
    "            distance_matrix=params.DISTANCE_MATRIX,\n",
    "            threshold=params.THRESHOLD\n",
    "        )\n",
    "\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'list_of_image_path' from 'face_recognitions.utils.common' (d:\\develop\\pipline\\face_recognitions\\src\\face_recognitions\\utils\\common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mface_recognitions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_yaml,list_of_image_path\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'list_of_image_path' from 'face_recognitions.utils.common' (d:\\develop\\pipline\\face_recognitions\\src\\face_recognitions\\utils\\common.py)"
     ]
    }
   ],
   "source": [
    "from face_recognitions.utils.common import read_yaml,list_of_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_recognitions import logger\n",
    "# from face_recognitions.utils.common import list_of_image_path\n",
    "from deepface.modules import representation, detection, modeling, verification\n",
    "from deepface.models.FacialRecognition import FacialRecognition\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Tuple\n",
    "import time\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self,config:Evaluation_Config):\n",
    "        self.config=config\n",
    "    \n",
    "    def face_name(self,path:Path)->str:\n",
    "        return os.path.basename(os.path.dirname(path))\n",
    "    \n",
    "    def list_of_image_path(self,path:Path)->List[Path]:\n",
    "\n",
    "        \"\"\"\n",
    "        List images in a given path\n",
    "        Args:\n",
    "            path (str): path's location\n",
    "        Returns:\n",
    "            images (list): list of exact image paths\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        for r, _, f in os.walk(path):\n",
    "            for file in f:\n",
    "                if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    exact_path = os.path.join(r, file)\n",
    "                    images.append(exact_path)\n",
    "        return images\n",
    "    \n",
    "    def find(self,\n",
    "             find_img_path:Path,\n",
    "             represent_model_df:pd.DataFrame,\n",
    "             target_size:Tuple[int, int],\n",
    "             threshold: Optional[float] = None,\n",
    "             )->List[pd.DataFrame]:\n",
    "        tic = time.time()\n",
    "\n",
    "        config=self.config\n",
    "        distance_metric=config.distance_matrix\n",
    "\n",
    "        # img path might have more than once face\n",
    "        source_objs = detection.extract_faces(\n",
    "            img_path=find_img_path,\n",
    "            target_size=target_size,\n",
    "            detector_backend=config.face_detector_backend,\n",
    "            grayscale=config.gray_scale,\n",
    "            enforce_detection=config.enforce_detection,\n",
    "            align=config.align,\n",
    "            expand_percentage=config.expand_percentage,\n",
    "        )\n",
    "\n",
    "        resp_obj = []\n",
    "\n",
    "        for source_obj in source_objs:\n",
    "            source_img = source_obj[\"face\"]\n",
    "            source_region = source_obj[\"facial_area\"]\n",
    "            target_embedding_obj = representation.represent(\n",
    "                img_path=source_img,\n",
    "                model_name=config.model_name,\n",
    "                enforce_detection=config.enforce_detection,\n",
    "                detector_backend=\"skip\",\n",
    "                align=config.align,\n",
    "                normalization=config.normalization,\n",
    "            )\n",
    "\n",
    "            target_representation = target_embedding_obj[0][\"embedding\"]\n",
    "\n",
    "            result_df = represent_model_df.copy()  # df will be filtered in each img\n",
    "            result_df[\"source_x\"] = source_region[\"x\"]\n",
    "            result_df[\"source_y\"] = source_region[\"y\"]\n",
    "            result_df[\"source_w\"] = source_region[\"w\"]\n",
    "            result_df[\"source_h\"] = source_region[\"h\"]\n",
    "\n",
    "            distances = []\n",
    "            for _, instance in represent_model_df.iterrows():\n",
    "                source_representation = instance[f\"{config.model_name}_representation\"]\n",
    "                if source_representation is None:\n",
    "                    distances.append(float(\"inf\")) # no representation for this image\n",
    "                    continue\n",
    "\n",
    "                target_dims = len(list(target_representation))\n",
    "                source_dims = len(list(source_representation))\n",
    "                if target_dims != source_dims:\n",
    "                    raise ValueError(\n",
    "                        \"Source and target embeddings must have same dimensions but \"\n",
    "                        + f\"{target_dims}:{source_dims}. Model structure may change\"\n",
    "                        + \" after pickle created. Delete the {file_name} and re-run.\"\n",
    "                    )\n",
    "\n",
    "                if distance_metric == \"cosine\":\n",
    "                    distance = verification.find_cosine_distance(\n",
    "                        source_representation, target_representation\n",
    "                    )\n",
    "                elif distance_metric == \"euclidean\":\n",
    "                    distance = verification.find_euclidean_distance(\n",
    "                        source_representation, target_representation\n",
    "                    )\n",
    "                elif distance_metric == \"euclidean_l2\":\n",
    "                    distance = verification.find_euclidean_distance(\n",
    "                        verification.l2_normalize(source_representation),\n",
    "                        verification.l2_normalize(target_representation),\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"invalid distance metric passes - {distance_metric}\")\n",
    "\n",
    "                distances.append(distance)\n",
    "\n",
    "                # ---------------------------\n",
    "            target_threshold = threshold or verification.find_threshold(config.model_name, distance_metric)\n",
    "\n",
    "            result_df[\"threshold\"] = target_threshold\n",
    "            result_df[\"distance\"] = distances\n",
    "\n",
    "            result_df = result_df.drop(columns=[f\"{config.model_name}_representation\"])\n",
    "            # pylint: disable=unsubscriptable-object\n",
    "            result_df = result_df[result_df[\"distance\"] <= target_threshold]\n",
    "            result_df = result_df.sort_values(by=[\"distance\"], ascending=True).reset_index(drop=True)\n",
    "            resp_obj.append(result_df)\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "        if not config.silent:\n",
    "            toc = time.time()\n",
    "            logger.info(f\"find function duration {toc - tic} seconds\")\n",
    "\n",
    "        return resp_obj\n",
    "   \n",
    "    def Eval(self):\n",
    "        tic = time.time()\n",
    "\n",
    "        config=self.config\n",
    "         # -------------------------------\n",
    "        if os.path.isdir(config.test_faces_dir) is not True:\n",
    "            raise ValueError(\"Passed test_faces_dir does not exist!\")\n",
    "        \n",
    "        test_list_image_path=self.list_of_image_path(config.test_faces_dir)\n",
    "\n",
    "        # -------------------------------\n",
    "        if  not os.path.exists(config.represent_file):\n",
    "            raise ValueError(\"Passed represent_file does not exist!\")\n",
    "\n",
    "        model: FacialRecognition = modeling.build_model(config.model_name)\n",
    "        target_size = model.input_shape\n",
    "\n",
    "        # ---------------------------------------\n",
    "\n",
    "        df_cols = [\n",
    "            \"identity\",\n",
    "            f\"{config.model_name}_representation\",\n",
    "            \"target_x\",\n",
    "            \"target_y\",\n",
    "            \"target_w\",\n",
    "            \"target_h\",\n",
    "            ]\n",
    "        \n",
    "        # Load the representations from the pickle file\n",
    "        with open(config.represent_file, \"rb\") as f:\n",
    "            representations = pickle.load(f)\n",
    "    \n",
    "            # Should we have no representations bailout\n",
    "        if len(representations) == 0 or len(representations)<0:\n",
    "            raise ValueError(f\"Retrain representation - size of representation {len(representations)}\")\n",
    "        \n",
    "        # now, we got representations for facial database\n",
    "        df = pd.DataFrame(\n",
    "            representations,\n",
    "            columns=df_cols,\n",
    "        )\n",
    "\n",
    "        evaluation_test_faces_result=[]\n",
    "        \n",
    "        for image_path in test_list_image_path[:10]:\n",
    "            result=self.find(\n",
    "                find_img_path=image_path,\n",
    "                represent_model_df=df,\n",
    "                target_size=target_size,\n",
    "            )\n",
    "            print(image_path)\n",
    "            evaluation_test_faces_result.append(result)\n",
    "        \n",
    "        # -----------------------------------\n",
    "\n",
    "        if not config.silent:\n",
    "            toc = time.time()\n",
    "            logger.info(f\"find function duration {toc - tic} seconds\")\n",
    "\n",
    "        return evaluation_test_faces_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-28 17:13:03,959: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-28 17:13:03,959: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-28 17:13:03,971: INFO: common: created directory at: artifacts]\n",
      "[2024-02-28 17:13:03,975: INFO: common: created directory at: artifacts/test]\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "12/12 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[2024-02-28 17:13:07,323: INFO: 2575025493: find function duration 3.1305601596832275 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\102\\7755.jpg\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "6/6 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[2024-02-28 17:13:10,262: INFO: 2575025493: find function duration 2.939180612564087 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\102\\8526.jpg\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[2024-02-28 17:13:13,395: INFO: 2575025493: find function duration 3.1324055194854736 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\102\\9988.jpg\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "6/6 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[2024-02-28 17:13:16,426: INFO: 2575025493: find function duration 3.0313968658447266 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\1027\\5195.jpg\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "[2024-02-28 17:13:19,445: INFO: 2575025493: find function duration 3.0019876956939697 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\1027\\5222.jpg\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[2024-02-28 17:13:22,411: INFO: 2575025493: find function duration 2.964899778366089 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\1027\\5338.jpg\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "11/11 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[2024-02-28 17:13:26,282: INFO: 2575025493: find function duration 3.8709752559661865 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\103\\4213.jpg\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "5/5 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[2024-02-28 17:13:29,381: INFO: 2575025493: find function duration 3.098661184310913 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\103\\554.jpg\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[2024-02-28 17:13:32,587: INFO: 2575025493: find function duration 3.205972909927368 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\103\\729.jpg\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[2024-02-28 17:13:35,638: INFO: 2575025493: find function duration 3.0351014137268066 seconds]\n",
      "artifacts/data_ingestion/dataset/test\\103\\7395.jpg\n",
      "[2024-02-28 17:13:35,638: INFO: 2575025493: find function duration 31.66119885444641 seconds]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    obj=ConfigurationManager()\n",
    "    aa=obj.get_eval_config()\n",
    "    bb=Evaluation(config=aa)\n",
    "\n",
    "    df=bb.Eval()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facerecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
